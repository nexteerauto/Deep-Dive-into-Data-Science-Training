{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783ea17a",
   "metadata": {},
   "source": [
    "## EDA Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e9762",
   "metadata": {},
   "source": [
    "> **Copyright Notice**    \n",
    "> \n",
    "> This IPython notebook is part of the **Deep Dive into Data Science** training program at Nexteer Automotive.  \n",
    "> It incorporates materials from **Coursera**'s **Deep Learning Specialization**, **TensorFlow: Advanced Techniques Specialization**, and **Mathematics for Machine Learning and Data Science** Specialization, licensed under the **Creative Commons Attribution-ShareAlike 2.0 (CC BY-SA 2.0)**, as well as other sources (including, but not limited to, enhancements developed with the assistance of generative AI tools).  \n",
    "> All original content created for this program, and all adaptations of source materials, are the intellectual property of Nexteer Automotive and are licensed under the same **Creative Commons Attribution-ShareAlike 2.0 (CC BY-SA 2.0)** license."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf507d",
   "metadata": {},
   "source": [
    "Conduct Exploratory Data Analysis (EDA) on the provided dataset using the process outlined in the last section of this module. All methods from the last section have been listed for you as a guidline. It is recommended to first decide which of those methods are applicable to this specific dataset, then proceed with the analysis.    \n",
    "\n",
    "**Reminder:** You may need to research how to implement some of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b72c6",
   "metadata": {},
   "source": [
    "### Loan - Credit Risk & Population Stability Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d454cd",
   "metadata": {},
   "source": [
    "Loan - Credit Risk & Population Stability is a part of Lending Club Company public database. LendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California. It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. LendingClub is the world's largest peer-to-peer lending platform.  \n",
    "\n",
    "Source: [Loan - Credit Risk & Population Stability - Kaggle](https://www.kaggle.com/datasets/beatafaron/loan-credit-risk-and-population-stability?select=loan_2014_18.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09b05b",
   "metadata": {},
   "source": [
    "**Download Dataset**: Please download `loan_2014_18.csv` from [here](https://www.kaggle.com/datasets/beatafaron/loan-credit-risk-and-population-stability?select=loan_2014_18.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e175e14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>purpose</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.97%</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>715.0</td>\n",
       "      <td>719.0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>CA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37%</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99%</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>155000.0</td>\n",
       "      <td>12.35</td>\n",
       "      <td>715.0</td>\n",
       "      <td>719.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>NJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.1%</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.05%</td>\n",
       "      <td>9 years</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>31.11</td>\n",
       "      <td>765.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>TX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.7%</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.97%</td>\n",
       "      <td>5 years</td>\n",
       "      <td>79077.0</td>\n",
       "      <td>15.94</td>\n",
       "      <td>700.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>VA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>57.7%</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.21%</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>107000.0</td>\n",
       "      <td>19.06</td>\n",
       "      <td>785.0</td>\n",
       "      <td>789.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>TX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.1%</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Late (31-120 days)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term int_rate emp_length  annual_inc    dti  \\\n",
       "0    12000.0   36 months    7.97%  10+ years     42000.0  27.74   \n",
       "1    32000.0   36 months   11.99%  10+ years    155000.0  12.35   \n",
       "2    40000.0   60 months   15.05%    9 years    120000.0  31.11   \n",
       "3    16000.0   36 months    7.97%    5 years     79077.0  15.94   \n",
       "4    33000.0   36 months    7.21%   < 1 year    107000.0  19.06   \n",
       "\n",
       "   fico_range_low  fico_range_high home_ownership             purpose  \\\n",
       "0           715.0            719.0            OWN  debt_consolidation   \n",
       "1           715.0            719.0       MORTGAGE         credit_card   \n",
       "2           765.0            769.0       MORTGAGE  debt_consolidation   \n",
       "3           700.0            704.0           RENT  debt_consolidation   \n",
       "4           785.0            789.0       MORTGAGE  debt_consolidation   \n",
       "\n",
       "  addr_state  inq_last_6mths  open_acc revol_util  total_acc  \\\n",
       "0         CA             0.0       9.0        37%       16.0   \n",
       "1         NJ             1.0      20.0      34.1%       42.0   \n",
       "2         TX             0.0      12.0      20.7%       26.0   \n",
       "3         VA             0.0      12.0      57.7%       20.0   \n",
       "4         TX             0.0      25.0      16.1%       52.0   \n",
       "\n",
       "          loan_status  \n",
       "0          Fully Paid  \n",
       "1             Current  \n",
       "2             Current  \n",
       "3             Current  \n",
       "4  Late (31-120 days)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv(\"loan_2014_18.csv\", low_memory=False)\n",
    "df_raw = df_raw.drop(df_raw.columns[0], axis=1)\n",
    "\n",
    "selected_columns = [\n",
    "    \"loan_amnt\",\n",
    "    \"term\",\n",
    "    \"int_rate\",\n",
    "    \"emp_length\",\n",
    "    \"annual_inc\",\n",
    "    \"dti\",\n",
    "    \"fico_range_low\",\n",
    "    \"fico_range_high\",\n",
    "    \"home_ownership\",\n",
    "    \"purpose\",\n",
    "    \"addr_state\",\n",
    "    \"inq_last_6mths\",\n",
    "    \"open_acc\",\n",
    "    \"revol_util\",\n",
    "    \"total_acc\",\n",
    "    \"loan_status\"  # Target variable\n",
    "]\n",
    "\n",
    "df = df_raw[selected_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c317f",
   "metadata": {},
   "source": [
    "### Understand Data Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ed571",
   "metadata": {},
   "source": [
    "*   **Initial Data Profiling:** Generate a first-pass summary: data types, unique values (cardinality), missing counts/percentages, basic descriptive statistics, memory usage, duplicate rows. *Start with manual inspection (`.head()`, `.info()`, `.describe()`).*  \n",
    "\n",
    "*   **Detailed Variable Characterization (Univariate Focus):**\n",
    "    *   **Univariate Analysis:** Analyze key variables individually (prioritizing target and hypothesized important predictors).\n",
    "        *   **Visualization:** Use appropriate plots (Histogram, Boxplot/Violin, Bar Chart, Density Plot). *Interpret plots in context.*\n",
    "        *   **Descriptive Statistics:** Calculate central tendency, spread, shape. *Relate stats to visualizations.*\n",
    "        * **Evaluate Distributions:** Rigorously assess the distribution of numerical variables.\n",
    "            * **Q-Q Plot:** Visual check for normality against a theoretical distribution.\n",
    "            * **Tests for Normality:**\n",
    "                * **Anderson-Darling Test:** More sensitive to deviations in the tails of the distribution.\n",
    "                * **Kolmogorov-Smirnov Test:** Compares the empirical distribution function to a theoretical distribution.\n",
    "                * If $\\mu$ and $\\sigma$ are estimated from sample, use *Lilliefors Test* instead of KS.\n",
    "            * **Choosing Between KS and AD**:\n",
    "                * If you suspect deviations in the central part of the distribution, the KS test might be a good choice due to its simplicity and sensitivity in that region.\n",
    "                * If you're concerned about outliers or heavy tails, the AD test is generally more powerful as it emphasizes discrepancies in these areas.\n",
    "            *   **Tests for Homogeneity of Variance:** (If comparing groups) Levene's (robust), Bartlett's (requires normality).\n",
    "            *   Analyze the distribution of *each* variable, understand its characteristics (e.g., skewed, multimodal, heavy-tailed) and its implications for analysis and modeling.\n",
    "    *   **Target Variable Specific Analysis:** Thoroughly analyze target distribution (regression: skewness, range; classification: class balance). *Crucial for model choice, metrics, sampling.*\n",
    "\n",
    "*   **Explore Data Structures (If Applicable):** Analyze inherent structures:\n",
    "    *   **Time Series:** Trends, seasonality, cycles, autocorrelation (ACF/PACF), stationarity (e.g., ADF/KPSS tests).\n",
    "    *   **Spatial Data:** Patterns, clustering, spatial autocorrelation (e.g., Moran's I).\n",
    "    *   **Hierarchical/Nested Data:** Structure, dependencies, intra-class correlation (ICC).\n",
    "    *   **Network/Graph Data:** Node degrees, centrality, communities.  \n",
    "\n",
    "*   **Initial Bivariate Exploration:** Quick check of relationships between key predictors & target, and among key predictors (e.g., scatter plots, grouped boxplots) *to inform cleaning and refine hypotheses early.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3b8b0",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030810d9",
   "metadata": {},
   "source": [
    "* **Handle Missing Values:**\n",
    "    * **Identify Missingness Mechanism:** Use formal tests like Little's MCAR test or visualize patterns of missingness (e.g., matrix plots, heatmaps of missingness) to understand if data is Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR) [Read more [here](https://medium.com/analytics-vidhya/different-types-of-missing-data-59c87c046bf7)]. This informs the handling strategy.\n",
    "        * Make sure assumptions hold. Little's MCAR assumes normality.\n",
    "        * For large datasets, assess missingness via\n",
    "            * *Visualization:* (e.g., heatmaps, missingness matrices using libaries like `missingno` in Python) as a quick and intuitive alternative for computationally expensive methods like Little's MCAR.\n",
    "            * *Summary Statistics:* Examine descriptive statistics (means, variances) for observed and missing data patterns can reveal potential systematic differences.\n",
    "            * *Creating Missingness Indicators:* Create binary indicator variables for each variable with missing data (e.g., `is_missing_variable_A`), then analyze the relationships between these indicators and other observed variables using techniques like logistic regression. Significant relationships suggest that the missingness is likely not MCAR (Missing Completely At Random).\n",
    "            * *Focus on Robustness of Models:* Instead of exhaustively testing the missingness mechanism, focus on using imputation techniques or modeling approaches that are robust to different types of missingness.\n",
    "    * **Strategize Imputation or Removal:** Based on the missingness mechanism, the percentage of missing data, and the problem context, choose appropriate strategies. \n",
    "        * If the proportion of missing values is very low, remove rows/columns (use with caution)\n",
    "        * Otherwise, use the appropriate imputation techniques e.g., simple imputation (mean, median, mode, constant), or more sophisticated methods (K-Nearest Neighbors imputation, model-based imputation like MICE - Multivariate Imputation by Chained Equations).\n",
    "* **Handle Outliers:**\n",
    "    * **Identify Outliers:** Use statistical methods like Z-score (for normally distributed data), IQR (robust to non-normality), or more advanced techniques tailored to the data distribution or problem (e.g., robust statistical methods, isolation forests, domain-specific rules).\n",
    "    * **Analyze Impact and Strategize Treatment:** Assess the potential impact of identified outliers on descriptive statistics, distributions, and relationships. Decide on appropriate actions based on the problem context and the likely cause of the outliers (e.g., removal if clear errors, capping/winsorizing to limit extreme values, transformation to reduce their influence, or using models that are robust to outliers).\n",
    "* **Address Inconsistent Data:** Identify and correct inconsistencies in data entry (e.g., variations in spelling), units (e.g., mixed metric and imperial), or formatting (e.g., date formats).\n",
    "* **Handle Duplicates:** Identify and handle duplicate records based on defined criteria. Decide on appropriate method to handle them (keep first/last, remove all, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c5167",
   "metadata": {},
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae21010",
   "metadata": {},
   "source": [
    "* **Apply Distributional Transforms:** Based on the analysis in Step 1 (Understand Data Distribution), apply transformations to address skewness or other distributional issues if required by potential modeling techniques that assume specific data distributions (e.g., log transformation, square root, reciprocal, Box-Cox transformation).\n",
    "* **Handle Categorical Variables:**\n",
    "    * **Analyze Cardinality:** Assess the number of unique values in categorical features. High cardinality requires careful consideration.\n",
    "    * **Choose Encoding Strategies:** Select appropriate encoding methods based on cardinality, whether the variable is nominal or ordinal, and the requirements of the potential modeling technique. Common methods include One-Hot Encoding, Ordinal Encoding, and Target Encoding (use with caution to avoid leakage). Consider strategies for high-cardinality features like grouping rare categories or using feature hashing.\n",
    "* **Address Data Scaling:** Scale numerical features to a common range to ensure all features contribute equally during model training (e.g., distance-based algorithms like K-Nearest Neighbors, SVMs, or algorithms using gradient descent):\n",
    "    * Linear scaling (Min-Max)\n",
    "    * Log scaling\n",
    "    * Z-score scaling\n",
    "    * Clipping  \n",
    "\n",
    "    Read more on choosing the appropriate methods [here](https://developers.google.com/machine-learning/data-prep/transform/normalization).  \n",
    "\n",
    "* **Handle Imbalanced Data (Recognition & Planning):** *Recognize* imbalance during analysis and *Plan* for addressing it during modeling (next phase) through techniques like resampling (oversampling, undersampling, SMOTE, etc.) or using appropriate loss functions/evaluation metrics. Read more treating imbalanced datasets [here](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a350dd",
   "metadata": {},
   "source": [
    "**Reference (Address Data Scaling)**: [Machine Learning Crash Course - Numberical data: Normalization](https://developers.google.com/machine-learning/crash-course/numerical-data/normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454ab0f",
   "metadata": {},
   "source": [
    "### Gain Deeper Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36bfb8",
   "metadata": {},
   "source": [
    "* **Analyze Relationships:** Explore predictor-target and predictor-predictor relationships.\n",
    "    * **Predictor-Target:**  Use visuals (scatter, grouped boxplots, bar plots). Assess statistical relationships:\n",
    "        *   *Correlation Coefficients*: Pearson (linear, normal data) or Spearman (robust to non-normality/non-linear monotonic).\n",
    "        *   *t-Tests*: Compare means for a continuous target across two groups (binary predictor).\n",
    "        *   *ANOVA*: Compare means for a continuous target across >2 groups (categorical predictor); F-test assesses overall model usefulness in regression.\n",
    "        *   *Chi-Square Test*: Assess association between two categorical variables. \n",
    "    * **Predictor-Predictor:** Correlation matrices (heatmaps), pairs plots.\n",
    "    * **Assess Multicollinearity:** Evaluate harmful multicollinearity among predictor variables, especially for *linear* models, using Variance Inflation Factor (VIF). High VIF values (commonly >5 or 10) indicate a feature is highly predictable by others. Address if necessary (remove/combine features, use regularization)  \n",
    "* **Feature Selection & Importance Analysis:** Inform feature choice by assessing potential predictive power.\n",
    "    * **Use Dimensionality Reduction for Visualization and Insight:** Use PCA/t-SNE for insight into patterns/clusters. *Interpret components derived from methods like PCA if possible, as they represent combinations of original features contributing most to data variance.*  \n",
    "    * **Analytical Importance Metrics:** Use metrics derived from statistical tests (p-values from t-tests, ANOVA, Chi-Square tests), correlation magnitudes, or simple univariate ranking methods (e.g., SelectKBest based on statistical scores).\n",
    "    * **Model-Based Importance Analysis:** \n",
    "        * *Stepwise Model Selection*: Iteratively add or remove features based on statistical criteria like AIC or BIC, commonly used in regression.\n",
    "        * *LASSO Regression*: In a regression context, LASSO (L1 regularization) can automatically shrink coefficients of less important features towards zero, effectively performing feature selection. Examining which coefficients remain non-zero provides insight into feature importance.\n",
    "        * *Tree-Based Importance*: Metrics like Gini Index or Information Gain are useful for ranking features based on how well they split data in decision tree-based models.\n",
    "        * *Ablation Analysis*: Often used in deep learning, conceptually involves removing or permuting a feature to see the impact on model performance, which can be applied analytically using a simple model to gauge importance.\n",
    "        * *Recursive Feature Elimination (RFE)*: Techniques like RFE can also be used with a simple model as an analytical tool to rank features.\n",
    "*   **Feature Engineering:** Create new, potentially more informative features from existing ones based on analysis.\n",
    "    *   Run residual diagnosis (in regression context), address distribution based issues by applying transformations (e.g., polynomial terms, log).\n",
    "    *   Explore if one predictor's relationship with the target changes based on another's value. Create interaction terms ($x \\times y$) based on identified potential interactions, esp. for regression. Explore statistically (preliminary regression with interaction terms).\n",
    "    *   Aggregate information (counts, ratios, stats) based on groups or windows.\n",
    "    *   Extract components from date/time features (trend, seasonality, cycles, etc.).\n",
    "    *   Discretize or bin continuous variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-dive-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
